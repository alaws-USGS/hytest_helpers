# uses fsspec 

# save files locally
# pandas dataframe to parquet
df.to_parquet("df.parquet")

# xarray dataset with dask to NETCDF
ds.to_netcdf("ds.nc", compute=True, mode='w', engine='h5netcdf')

# if not already using OSN profile
client.close(); cluster.shutdown()

# setup aws profile and start dask client/cluster
import configparser
## Read in and parse the AWS config file... 
awsconfig = configparser.ConfigParser()
awsconfig.read(
    os.path.expanduser('~/.aws/credentials') # default location... if yours is elsewhere, change this.
)

_profile_nm  = 'osn-renci'
_endpoint = 'https://renc.osn.xsede.org'
# Set environment vars based on parsed awsconfig
os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[_profile_nm]['aws_access_key_id']    
os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[_profile_nm]['aws_secret_access_key']    
os.environ['AWS_S3_ENDPOINT']       = _endpoint

try: 
    # Obliterate any reference to a profile.  
    # From here on in, we rely on the ACCESS_KEY_ID and SECRET_ACCESS_KEY established above. 
    del os.environ['AWS_PROFILE']
except KeyError:
    pass

from dask_gateway import Gateway
gateway = Gateway()
options = gateway.cluster_options()
options.conda_environment='users/pangeo'
options.profile = 'Medium Worker'

# pass environment vars to workers
# this includes AWS environment vars needed to access requester-pays and private buckets
options.environment_vars = dict(os.environ)
cluster = gateway.new_cluster(options)
cluster.adapt(minimum=2, maximum=30)

# get the client for the cluster
client = cluster.get_client()
client.dashboard_link

# create write FS
fs_write = fsspec.filesystem('s3',profile='osn-renci',skip_instance_cache=True, 
client_kwargs={'endpoint_url':'https://renc.osn.xsede.org'})

# use put to transfer file from local to OSN
_ = fs_write.put("df.parquet", "file/path/to/df.parquet")

# check for success
fs_write.ls("ile/path/to/", detail=True)
